,Unnamed: 0,start,end,text,best_match,similarity_score
5,5,75.6,92.6, so the fourth thing arises from the fact that you know if transactions commit in certain kind of order then they then it is possible for us to get into for the database to get it right up," The fourth consideration arises from the fact that transactions can commit in a specific order, allowing the database to be updated correctly",0.40758452941539364
7,7,103.6,114.6, meanwhile some so this is the case of the reading dirty data so mean where some other transaction s has read that item x completed its work," This scenario illustrates the case of reading dirty data, where transaction S may access the item before its modification is committed, leading to inconsistent data",0.40035010249696185
15,15,196.6,214.6, so the transaction is this schedule is called recoverable if in that schedule no transaction commits until all the transactions per much it has read have actually committed," Here is an extractive summary of the input:

In a recoverable transaction schedule, no transaction commits until all the transactions it has read have actually committed",0.7338878011108323
18,18,231.6,238.6, so if all of them commit then they can go ahead and then ask for a commit because it really can treat work," Only when all the transactions it has read have completed can transaction T proceed to commit, as it can then treat its work as durable",0.41315695227557747
22,22,285.6,302.6, so cascading rollbacks may occur but then without this property being satisfied by schedules the transit the database would not be able to give durability assurance at all," Without this property, the database cannot provide durability assurance",0.4360639116014995
23,23,302.6,313.6, so what durable the requires is that if some transaction commits then the effects of that particular transaction are permanently recorded in the database on the desk," This means that if a transaction commits, its effects must be permanently recorded in the database",0.5398856925293908
26,26,345.6,364.6, so in that context we have discussed the issues and then we have identified the notion of serializability and one particular way of achieving that serializability called configs serializability which is again achieved through block based protocol," Two key concepts were discussed: serializability and config serializability, which is achieved through a block-based protocol",0.4922853207350378
27,27,364.6,374.6, so we have discussed two things the locking protocol and if transactions follow two ways of locking protocol then serializability is guaranteed," Additionally, the locking protocol and its relationship to transaction locking were explored, including the two ways transactions can be locked",0.4074646007170086
28,28,374.6,384.6, but then these we realize that serializability does not you know put any restrictions on the order in which commits have to happen and things like that," However, it does not impose any restrictions on the order in which commits must occur",0.4732449488638799
31,31,413.6,425.6, so we actually all the desirable properties are the especially all the asset properties are from the icila collection and you know, The asset properties are derived from the ICILA collection,0.6823769848553078
36,36,467.6,477.6, this cascading list schedules are those kind of schedules in which we do not have this need for cascading role back," Here is an extractive summary of the input:

The concept of Cascading List Schedules (CLS) is introduced to eliminate the need for cascading rollbacks",0.4148444024576345
39,39,491.6,502.6, so schedule is called cascading list or acr schedule if in this in the schedule transactions we values written by committed transactions," Here is an extractive summary of the input:

The schedule transactions involve values written by committed transactions",0.5016179775958163
40,40,502.6,511.6, so you can reading some item i must ensure that it has been class written by or whatever role that last is a committed transaction," To ensure that items being read have been class-written by or have a role from a committed transaction, a clever modification to the working protocol is necessary",0.42981824762791565
42,42,514.6,531.6, so we have to cleverly again modify the our working protocol in such a way that when transactions are running they will not be allowed to be items that have not been committed by that have been written by," To ensure that items being read have been class-written by or have a role from a committed transaction, a clever modification to the working protocol is necessary",0.40098655743470757
46,46,559.6,572.6, so such a so a schedule in this schedule every transaction surely you know will commit only after all the transactions that have that it has read from commit," This ensures that every transaction will commit only after all the transactions it has read from have committed, making the schedule recoverable",0.682416130434037
56,56,640.6,652.6, so so for locking based methods for concurrency control notice that there are other ways of achieving concurrency control especially we have who we did not have time to discuss this other," Additionally, there are other methods for concurrency control beyond 2PL",0.4637147947114174
58,58,658.6,675.6, these time stamp based concurrency control are actually also called optimistic concurrency control techniques in a sense that they will they will let the transactions run and then check whether everything is proper," Here is an extractive summary of the input:

Optimistic concurrency control techniques, also known as time-stamp based approaches, allow transactions to run and then check if everything is proper, rather than taking corrective action",0.5225047599109298
62,62,685.6,700.6, whereas the locking based concurrency control based on two case locking protocol is a pessimistic approach it will assume that if you do not do anything transactions will conflict with each other," Here is an extractive summary:

The two-case locking protocol is a pessimistic approach to concurrency control, assuming that transactions will conflict with each other if not controlled",0.6126668884090761
63,63,700.6,715.6, and so it is it is enforcing conflict serializability and thus making sure that the schedules that arrive are conflict equal and to some serial schedule serial schedules are same," This protocol enforces conflict serializability, ensuring that schedules are conflict-equivalent to a serial schedule, which is a schedule that can be executed without conflicts",0.4639590413767946
66,66,740.6,757.6, so we will we will impose that a transaction must not release any locks especially right locks right locks are those which are acquired for the purpose of writing updating the right," Specifically, any locks acquired for the purpose of writing or updating (right locks) must not be released until the transaction has completed its commit or rollback operation",0.5557478079832501
69,69,773.6,784.6, so if you do not release locks if a transaction does not release locks then there is no chance for any other transaction to acquire a lock on some item and then it will serve right," Here is an extractive summary of the input:

When a transaction does not release locks, other transactions cannot acquire locks on the same items, and the transaction will hold all locks until it commits or rolls back",0.44568921210571133
74,74,837.6,855.6, so this modification results in what are called strict schedules so strict schedules are those schedules in which the transaction may the weeds now writes an item until the last transaction the road acts as has been eliminated," In strict schedules, a transaction can only write an item until the last transaction has been committed, eliminating the possibility of conflicts",0.4405482421505361
76,76,860.6,887.6, so this will result in what are called strict schedules and we can we can so strict schedules are following through pl is a strict to pl is more strict version of 2 pairs locking why is this strict version of 2 pairs locking because in 2 pairs locking all that we said is that dont ask for a lock the moment you start issuing and locks," Here is an extractive summary of the input:

The topic discusses strict schedules, which are a stricter version of 2 pairs locking",0.45937214115636504
83,83,981.6,1004.6, so there are 2 access more we dont need access more and we write access more the default is read right access more and isolation level is something you know so in practice it is of course a theoretically all these schedules have to be serializable schedules," Access Mode has two settings: ""don't need access more"" and ""write access more"", with the default being ""read-only access more""",0.6707058843381003
85,85,1009.6,1022.6, but in practice sql would like to give the users an option of running their transactions in a slightly relaxed isolation level," However, in reality, SQL provides users with the option to run transactions at a slightly relaxed isolation level, known as ""read uncommitted"" or ""dirty read"" isolation level, which is less restrictive than the default serializable level",0.42323313192398676
89,89,1046.6,1057.6, so actually these are meant for running transactions that kind of collect statistics and aggregate something like that where you are not really so bothered about the up to date values and things like that," They are typically used for transactions that collect statistics and aggregate data, where up-to-date values are not crucial",0.538349027129436
92,92,1071.6,1083.6, its actually dangerous so sql will take an extra precaution to allow only read only transactions to run at such low isolation level," A read-only transaction can run at a low isolation level, such as ""read uncommitted"", which allows it to read data that has been modified but not yet committed",0.4069675731765072
96,96,1108.6,1114.6, so the highest isolation level is serializability schedules," The highest isolation level is ""serializability"", which ensures that transactions are executed in a way that maintains data consistency",0.4666384302272835
99,99,1126.6,1132.6, so this is the lowest level of isolation that one can use but this is the most dangerous level," Here is an extractive summary:

Read Uncommitted is the lowest level of isolation in SQL, which allows for the most dangerous level of concurrency",0.5778064472587472
100,100,1132.6,1138.6, so sql allows you only to run read only transactions in this mode," In this mode, only read-only transactions are allowed, and the access mode is automatically set to read-only",0.6252394395351969
101,101,1138.6,1144.6, so access mode will be automatically set to read only if you choose beside the isolation level," In this mode, only read-only transactions are allowed, and the access mode is automatically set to read-only",0.5982504910531826
104,104,1159.6,1168.6, then the next highest level is read higher level is read committed where dirty reads do not occur but unrepeatable reads might occur," Here is an extractive summary:

The next level of isolation is ""read committed"", where dirty reads do not occur, but unrepeatable reads might",0.8043080018462577
106,106,1174.6,1186.6, as repeatable reads no dirty data will be read and no all the reads are repeatable and but pantons might occur," This is because repeatable reads are possible, but PANTONs (partially-ordered non-atomic transactions) might occur",0.45173262124185476
108,108,1189.6,1201.6, and the of course the best thing to do is to run your transaction at serializability level in which you are guaranteed that none of this problem arise," To avoid these issues, it's recommended to run transactions at a serializability level, which guarantees that none of these problems arise",0.41122684042789837
109,109,1202.6,1208.6, so the problem of pantomases is something which is like this," Here is an extractive summary of the problem of pantomases:

The problem of pantomases occurs when a transaction (T1) selects a set of tuples based on a specific condition, locks them, and starts processing them",0.4195511377722372
113,113,1240.6,1245.6, so this record actually was not even existing in the database when the t1 started," Meanwhile, another transaction (T2) comes along and updates the database with a new tuple that satisfies the same condition, even though it was not present in the database when T1 started",0.428899510716607
